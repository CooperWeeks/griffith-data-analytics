{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling for News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1495020689067-958852a7765e?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)\n",
    "\n",
    "Photo by [Roman Kraft](https://unsplash.com/photos/_Zua2hyvTBk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is about modelling the main topics of a database of News headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing the needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Coope/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/Coope/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Coope/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Coope/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# TODO: import needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2csc\n",
    "from gensim.models import TfidfModel, LsiModel, LdaModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data in the file `random_headlines.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20120305</td>\n",
       "      <td>ute driver hurt in intersection crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20081128</td>\n",
       "      <td>6yo dies in cycling accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090325</td>\n",
       "      <td>bumper olive harvest expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20100201</td>\n",
       "      <td>replica replaces northernmost sign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20080225</td>\n",
       "      <td>woods targets perfect season</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                          headline_text\n",
       "0      20120305  ute driver hurt in intersection crash\n",
       "1      20081128           6yo dies in cycling accident\n",
       "2      20090325          bumper olive harvest expected\n",
       "3      20100201     replica replaces northernmost sign\n",
       "4      20080225           woods targets perfect season"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: load the dataset\n",
    "df = pd.read_csv(\"random_headlines.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is always a good idea to perform some EDA (exploratory data analytics) on a dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   publish_date   20000 non-null  int64 \n",
      " 1   headline_text  20000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 312.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform a short EDA\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform all the needed preprocessing on those headlines: case lowering, tokenization, punctuation removal, stopwords removal, stemming/lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Preprocess the input data\n",
    "ColumnName = \"headline_text\"\n",
    "filtered_tokens = [[PorterStemmer().stem(token) for token in word_tokenize(sentence) \n",
    "                    if token.lower() not in set(string.punctuation) and \n",
    "                    token not in set(stopwords.words(\"english\"))] \n",
    "                    for sentence in df[ColumnName]]\n",
    "filtered_Df = pd.DataFrame({\"stemmed\":filtered_tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the BOW using Gensim\n",
    "dictionary = Dictionary(filtered_tokens)\n",
    "bow_corpus = [dictionary.doc2bow(tokens) for tokens in filtered_tokens]\n",
    "bow_df = pd.DataFrame(corpus2csc(bow_corpus).toarray().transpose(), \n",
    "         columns=[dictionary[i] for i in range(len(dictionary))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute TF-IDF\n",
    "tfidf_model = TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Coope/anaconda3/lib/python3.11/site-packages/gensim/models/lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
      "  sparsetools.csc_matvecs(\n",
      "/Users/Coope/anaconda3/lib/python3.11/site-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
      "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
      "/Users/Coope/anaconda3/lib/python3.11/site-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
      "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
      "/Users/Coope/anaconda3/lib/python3.11/site-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
      "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.458*\"man\" + 0.390*\"polic\" + 0.316*\"charg\" + 0.148*\"court\"\n",
      "Topic 1: -0.435*\"second\" + -0.410*\"90\" + -0.339*\"abc\" + -0.301*\"news\"\n",
      "Topic 2: -0.379*\"man\" + -0.272*\"charg\" + -0.262*\"second\" + -0.252*\"90\"\n",
      "Topic 3: -0.773*\"polic\" + 0.232*\"man\" + 0.222*\"charg\" + -0.151*\"probe\"\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute LSA\n",
    "num_topics = 4\n",
    "lsa_model = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "corpus_lsa = lsa_model[corpus_tfidf]\n",
    "\n",
    "# TODO: Print the 3 or 4 most significant words of each topic\n",
    "num_words = 4\n",
    "topics = lsa_model.print_topics(num_topics=num_topics, num_words=num_words)\n",
    "\n",
    "for topic_num, topic in topics:\n",
    "    print(f\"Topic {topic_num}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.023*\"polic\" + 0.017*\"man\" + 0.012*\"charg\" + 0.010*\"court\"\n",
      "Topic 1: 0.009*\"call\" + 0.008*\"interview\" + 0.008*\"govt\" + 0.008*\"fund\"\n",
      "Topic 2: 0.010*\"plan\" + 0.009*\"council\" + 0.007*\"mine\" + 0.006*\"chang\"\n",
      "Topic 3: 0.011*\"us\" + 0.007*\"open\" + 0.006*\"win\" + 0.006*\"new\"\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute LDA\n",
    "lda_model = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=42)\n",
    "corpus_lda = lda_model[bow_corpus]\n",
    "\n",
    "# TODO: Print the 3 or 4 most significant words of each topic\n",
    "topics = lda_model.print_topics(num_words=4)\n",
    "for topic_num, topic in topics:\n",
    "    print(f\"Topic {topic_num}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the LDA visualization data\n",
    "lda_vis_data = gensimvis.prepare(lda_model, bow_corpus, dictionary)\n",
    "\n",
    "# Save the visualization as an HTML file\n",
    "pyLDAvis.save_html(lda_vis_data, 'lda_visualization.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"lda_visualization.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x40ed87690>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the saved HTML file in a Jupyter notebook\n",
    "IFrame('lda_visualization.html', width=800, height=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your results, you can try to fine tune the algorithm: number of topics, hyperparameters...\n",
    "And check with others their results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
